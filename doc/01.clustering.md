# クラスタリング

* clusterは群れ，集合であり，類似データをまとめること
* 「教師なし学習」の代表的手法
* 外的分離：違うクラスタにある対象は似ていない
* 内的結合：同じクラス内の対象は互いに似ている



## クラスタリングの手法

1. **階層的クラスタリング**
   * 類似データを小さなクラスタにまとめ，そのクラスタ同士をまとめてより大きなクラスタを形成する．
   * それを繰り返しながらクラスタを構成する．
   * 計算量は必然的に多くなる
   * ボトムアップ的な手法である
2. **非階層的クラスタリング**
   * クラスタリング基準(目的関数)を設けて，最適化問題として分割を求める.
   * 目的関数の最適値を与える分割 = 最適解
   * 平方和最小基準基準クラスタリング
     * k-meansアルゴリズム
     * 



## 平方和最小基準クラスタリング

* 平方和基準：クラスタごとに求めた平方和を足し合わせたもの
* 小さければ小さいほど良い
* k-meansアルゴリズムが代表例



### クラスタリング手法

* クラスタリングの対象を入力ベクトルの集合とする
  N個の入力ベクトル $x$ をK個のクラスタ $C^k$ に分割する.
  $$
  \boldsymbol{x}=x_i(i=1,2,...,N) \\
  C^k(k=1,2,...,k)
  $$
  
* クラスの重心を $μ$ とする．
  
  $$
  μ_k = \frac{1}{N_k}\sum_{x_i \in C^k} x_i
  $$

* ユーグリッド2乗距離
  重心とのそれぞれの距離の二乗
  
  $$
  ||\boldsymbol{x}-\boldsymbol{μ}||^2=||\boldsymbol{μ}-\boldsymbol{x}||^2
  $$

* あるクラスタ $C^k$ の平方和
  
  $$
  \sum_{x_i \in C^k}||\boldsymbol{x}-\boldsymbol{μ}||^2=\sum_{x_i \in C^k}\sum_{m=1}^{M}(x_m^i - μ_m^k)^2
  $$

* クラスタ内平方和 $J_w$ (分割したクラスタの平方和をすべて足し合わせたもの)

  $$
  J_w=\sum_{k=1}^K(クラスタC^kの平方和) \\
  = \sum_{k=1}^K(\sum_{x_i \in C^k}\sum_{m=1}^{M}(x_m^i - μ_m^k)^2)
  $$



## k-meansアルゴリズム

* 機械学習で用いられるクラスタリング手法で最も有名
* 非階層的クラスタリング
* 最適化問題の解を求める
* `means`は「平均」の意味



### アルゴリズム

1. クラスタ数( グループ数, いくつに分けるか )を定義して，その数だけ
   代表となるベクトルを決定する
   決定方法はいろいろあるが，クラスタの重心で後から更新するので，正味どこでもええねん
2. データ数の分だけ，クラスタの重心に最も近いクラスタ(の重心ベクトル)に分類していく
   最も近いかどうかは，高校生の数Ⅱでも習うユーグリット2乗距離で判定する
3. すべてのデータがどのクラスタに属すかを確定したので，
   そのクラスタに属するデータたちの重心ベクトル(データの平均値)を次の代表ベクトルにするよ～！
4. (2.)に戻って(2～3を繰り返す) 繰り返し回数や終了条件はk-meansのアルゴリズムによる
   よくある終了条件 (3.)で「代表ベクトルと重心ベクトルが等しくなったら...」(クラスタのデータの平均値である重心ベクトルがその代表ベクトルを同じになったら....)
   * 更新されなかったということは，平均値がちょうどそのクラスタの中心(重心)にいるということである



## 実装

```python
# データ解析用のライブラリ
import pandas as pd
# 数値計算用のライブラリ (ベクトルとか行列とかを扱える，データ解析時には配列より扱いやすい)
import numpy as np
# scikit-learn : 機械学習用のライブラリ
# なんかええ感じのデータをええ感じに用意してくれよるわ
from sklearn.datasets import load_iris

# テキトウにサンプルデータを取得する
# とりまデフォで「150個」用意してくれとる
# ちな，アイリスはお花の名前よ，in Japan アヤメ
iris = load_iris()
#print(iris)
dataframe = pd.DataFrame(iris['data'], columns=iris.feature_names)
#print(dataframe)

# 4つのクラスタ数でk-meansを使って，クラスタリングしてみるサァ～～
from sklearn.cluster import KMeans
# fit_predict：各サンプルに対する、クラスタ番号を確定
km = KMeans(
    n_clusters=4,
	init='random')
pred = km.fit_predict(dataframe)
print(pred)


```



## refs

* https://qiita.com/kotai2003/items/ca429e21ed67166fd1fc
* [k-meansの最適なクラスター数を調べる方法](https://qiita.com/deaikei/items/11a10fde5bb47a2cf2c2)
* 


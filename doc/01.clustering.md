# クラスタリング

* clusterは群れ，集合であり，類似データをまとめること
* 「教師なし学習」の代表的手法
* 外的分離：違うクラスタにある対象は似ていない
* 内的結合：同じクラス内の対象は互いに似ている



## クラスタリングの手法

1. **階層的クラスタリング**
   * 類似データを小さなクラスタにまとめ，そのクラスタ同士をまとめてより大きなクラスタを形成する．
   * それを繰り返しながらクラスタを構成する．
   * 計算量は必然的に多くなる
   * ボトムアップ的な手法である
2. **非階層的クラスタリング**
   * クラスタリング基準(目的関数)を設けて，最適化問題として分割を求める.
   * 目的関数の最適値を与える分割 = 最適解



## 平方和最小基準クラスタリング

* 平方和基準：クラスタごとに求めた平方和を足し合わせたもの
* 小さければ小さいほど良い



### クラスタリング手法

* クラスタリングの対象を入力ベクトルの集合とする
  N個の入力ベクトル $$x$$ をK個のクラスタ $$C^k$$ に分割する
  $$
  \boldsymbol{x}=x_i(i=1,2,...,N) \\
  C^k(k=1,2,...,k)
  $$
  
* クラスの重心を $$μ$$ とする．
  
  $$
  μ_k = \frac{1}{N_k}\sum_{x_i \in C^k} x_i
  $$

* ユーグリッド2乗距離
  重心とのそれぞれの距離の二乗
  $$
  ||\boldsymbol{x}-\boldsymbol{μ}||^2=||\boldsymbol{μ}-\boldsymbol{x}||^2
  $$

* あるクラスタ $$C^k$$ の平方和
  
  $$
  \sum_{x_i \in C^k}||\boldsymbol{x}-\boldsymbol{μ}||^2=\sum_{x_i \in C^k}\sum_{m=1}^{M}(x_m^i - μ_m^k)^2
  $$

* クラスタ内平方和$$ J_w$$ (分割したクラスタの平方和をすべて足し合わせたもの)

  
  $$
  J_w=\sum_{k=1}^K(クラスタC^kの平方和) \\
  = \sum_{k=1}^K(\sum_{x_i \in C^k}\sum_{m=1}^{M}(x_m^i - μ_m^k)^2)
  $$
  
  
  
  
  
  $$
  
  $$